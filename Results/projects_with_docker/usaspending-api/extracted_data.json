{
    "source_code_info": {
        "language": "Python",
        "framework": "Django",
        "dependencies": {
            "pyproject.toml": "[tool.pytest.ini_options]\nDJANGO_SETTINGS_MODULE = \"usaspending_api.settings\"\naddopts = \"--cov=usaspending_api\"\nmarkers = [\n    \"signal_handling: Mark all tests that import the signal library and invoke signals. This MUST be done on the main thread, and can cause errors if pytest-xdist subordinates parellel test sessions to background threads.\",\n\n    # These are \"auto\" marked based on fixture usage. See conftest.py pytest_collection_modifyitems\n    \"spark: Mark all tests using the spark fixture. Can be selected with -m spark or deselected with -m (not spark)\",\n    \"database: Mark all integration tests using a database. Can be selected with -m database or deselected with -m (not database)\",\n    \"elasticsearch: Mark all integration tests using Elasticsearch. Can be selected with -m database or deselected with -m (not elasticsearch)\",\n]\n\n[tool.coverage.run]\nomit = [\n    # tests themselves don't need coverage measurements\n    \"*/tests/*\",\n    # nor db migration scripts\n    \"*/migrations/*\",\n]\n\n[tool.coverage.report]\n# Regexes for lines to exclude from consideration\nexclude_lines = [\n    # Have to re-enable the standard pragma\n    \"pragma: no cover\"\n]\n\n[tool.black]\nline-length = 120\ntarget-version = ['py310']\nexclude = '/(\\.git|\\.venv|venv|migrations)/'\n"
        },
        "database": "Elasticsearch",
        "build_system": "poetry",
        "config_files": {
            "docker-compose.yml": "# See usages of this compose file and its commands in the README.md file\nversion: '3.7'\n\nvolumes:\n  local_pg_data:\n    driver: local\n  local_es_data:\n    driver: local\n\nservices:\n\n  usaspending-db:\n    command: postgres -c 'max_connections=500'\n    profiles:\n      - usaspending  # must pass --profile usaspending to docker-compose for this to come up, or run a service with one of these other profiles\n      - manage\n      - test\n      - ci\n    image: postgres:13.8-alpine\n    container_name: usaspending-db\n    volumes:\n      - type: volume\n        source: local_pg_data\n        target: /var/lib/postgresql/data\n    ports:\n      - ${USASPENDING_DB_PORT:-5432}:5432\n    environment:\n      POSTGRES_USER: ${USASPENDING_DB_USER:-usaspending}\n      POSTGRES_PASSWORD: ${USASPENDING_DB_PASSWORD:-usaspender}\n      POSTGRES_DB: ${USASPENDING_DB_NAME:-data_store_api}\n\n  usaspending-manage:\n    profiles:\n      - manage  # must pass --profile manage to docker-compose for this to come up, or use docker-compose run\n    image: usaspending-backend  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    build: .\n    container_name: usaspending-manage\n    volumes:\n     - .:/dockermount\n    depends_on:\n      - usaspending-db\n    # For an interactive shell, override this command with: docker-compose run --rm usaspending-manage python3 -u manage.py shell\n    command: python3 -u manage.py help\n    environment:\n      DJANGO_DEBUG: ${DJANGO_DEBUG}\n      DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      ES_HOSTNAME: ${ES_HOSTNAME}\n      DATA_BROKER_DATABASE_URL: postgresql://${BROKER_DB_USER}:${BROKER_DB_PASSWORD}@${BROKER_DB_HOST}:${BROKER_DB_PORT}/data_broker\n\n  usaspending-test:\n    profiles:\n      - test  # must pass --profile test to docker-compose for this to come up, or use docker-compose run\n    image: usaspending-testing  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    build:\n      context: .\n      dockerfile: Dockerfile.testing\n    container_name: usaspending-test\n    volumes:\n     - .:/dockermount\n     # Required to interact with host's docker daemon from within this running container,\n     # to spin up the data-act-broker-init-test-db container used for broker integration tests (see: conftest.broker_db_setup)\n     - /var/run/docker.sock:/var/run/docker.sock\n    depends_on:\n      - usaspending-db\n      - usaspending-es\n    command: make tests\n    environment:\n      DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      ES_HOST: ${ES_HOST}\n      ES_HOSTNAME: ${ES_HOSTNAME}\n      DATA_BROKER_DATABASE_URL: postgresql://${BROKER_DB_USER}:${BROKER_DB_PASSWORD}@${BROKER_DB_HOST}:${BROKER_DB_PORT}/data_broker\n      # Location in host machine where broker src code root can be found\n      DATA_BROKER_SRC_PATH: \"$PWD/../data-act-broker-backend\"\n      MINIO_HOST: ${MINIO_HOST}\n      DOWNLOAD_DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n\n  usaspending-ci:\n    profiles:\n      - ci  # must pass --profile ci to docker-compose for this to come up, or use docker-compose run\n    image: usaspending-backend  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    build: .\n    container_name: usaspending-ci\n    volumes:\n     - .:/dockermount\n     # Required to interact with host's docker daemon from within this running container,\n     # to spin up the data-act-broker-init-test-db container used for broker integration tests (see: conftest.broker_db_setup)\n     - /var/run/docker.sock:/var/run/docker.sock\n    depends_on:\n      - usaspending-db\n      - usaspending-es\n    command:\n      - sh\n      - -c\n      - |\n        printf \"==============\\nChecking code format:\\n\"\n        black --check --diff .\n        printf -- \"-------\\nChecking code syntax:\\n\"\n        flake8 && echo \"Successfully passed\"\n        printf -- \"-------\\nChecking API documentation files:\\n\"\n        python3 manage.py check_for_endpoint_documentation\n        printf -- \"-------\\nRunning unit tests:\\n\"\n        pytest --durations 50 --ignore-glob='**/tests/integration/*' --cov=usaspending_api --cov-report= --reuse-db -rsx\n        printf -- \"-------\\nRunning integration tests:\\n\"\n        pytest --durations 50 --override-ini=python_files='**/tests/integration/*' --cov=usaspending_api --cov-append --cov-report term --cov-report xml:coverage.xml --reuse-db -rsx\n    environment:\n      DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      ES_HOSTNAME: ${ES_HOSTNAME}\n      DATA_BROKER_DATABASE_URL: postgresql://${BROKER_DB_USER}:${BROKER_DB_PASSWORD}@${BROKER_DB_HOST}:${BROKER_DB_PORT}/data_broker\n      # Location in host machine where broker src code root can be found\n      DATA_BROKER_SRC_PATH: \"$PWD/../data-act-broker-backend\"\n\n  usaspending-api:\n    profiles:\n      - usaspending  # must pass --profile usaspending to docker-compose for this to come up\n    image: usaspending-backend  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    build: .\n    container_name: usaspending-api\n    volumes:\n      - .:/dockermount\n    ports:\n      - 8000:8000\n    depends_on:\n      - usaspending-db\n      - usaspending-es\n    restart: on-failure:3 # 3 max attempt, and then it will stop restarting\n    # Must wait on postgres db to be up (~9s)\n    command: /bin/sh -c \"sleep 9s; python3 -u manage.py runserver --verbosity 2 0.0.0.0:8000\"\n    environment:\n      DJANGO_DEBUG: ${DJANGO_DEBUG}\n      RUN_LOCAL_DOWNLOAD_IN_PROCESS: \"False\"\n      DB_SOURCE: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      DB_R1: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      DOWNLOAD_DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      ES_HOSTNAME: ${ES_HOSTNAME}\n\n  usaspending-bulk-download:\n    profiles:\n      - usaspending  # must pass --profile usaspending to docker-compose for this to come up\n    image: usaspending-backend  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    build: .\n    container_name: usaspending-bulk-download\n    restart: on-failure:5 # 5 max attempt, and then it will stop restarting. NOTE: bulk download errors will cause one failure+restart iterations\n    volumes:\n    - .:/dockermount\n    command: python3 manage.py download_sqs_worker\n    environment:\n      DJANGO_DEBUG: ${DJANGO_DEBUG}\n      DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n      DOWNLOAD_DATABASE_URL: postgres://${USASPENDING_DB_USER}:${USASPENDING_DB_PASSWORD}@${USASPENDING_DB_HOST}:${USASPENDING_DB_PORT}/data_store_api\n\n  usaspending-es:\n    profiles:\n      - usaspending  # must pass --profile usaspending to docker-compose for this to come up\n      - test\n      - ci\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.1.1\n    container_name: usaspending-es\n    environment:\n      - node.name=usaspending-es\n      - discovery.seed_hosts=usaspending-es\n      - cluster.initial_master_nodes=usaspending-es\n      - cluster.name=usaspending\n      - network.host=0.0.0.0\n      - bootstrap.memory_lock=true\n      - \"ES_JAVA_OPTS=-Xms1536m -Xmx1536m\"  # Ensure Docker is allocated plenty of memory, otherwise this will fail\n    # Inject plugin install, then resume with orignial entrypoint command\n    command: >\n      /bin/sh -c \"\n        if [ ! -d /usr/share/elasticsearch/plugins/mapper-murmur3 ]; then\n          # Certificate problem workaround when on VPN - wget without checking cert, then install from local filesystem\n          wget --no-check-certificate https://artifacts.elastic.co/downloads/elasticsearch-plugins/mapper-murmur3/mapper-murmur3-7.1.1.zip\n          ./bin/elasticsearch-plugin install file:///usr/share/elasticsearch/mapper-murmur3-7.1.1.zip\n        fi\n        /usr/local/bin/docker-entrypoint.sh\"\n    ulimits:\n      memlock:\n        soft: -1\n        hard: -1\n    volumes:\n    - type: volume\n      source: local_es_data\n      target: /usr/share/elasticsearch/data\n    ports:\n      - 9200:9200\n\n  usaspending-kibana-es:\n    profiles:\n      - usaspending  # must pass --profile usaspending to docker-compose for this to come up\n    image: docker.elastic.co/kibana/kibana-oss:7.1.1\n    container_name: usaspending-kibana-es\n    # ELASTICSEARCH_HOSTS should match the port for \"usaspending-es\"; value will need to be updated if using Windows\n    environment:\n      - ELASTICSEARCH_HOSTS=\"http://docker.for.mac.localhost:9200\"\n    ports:\n      - 5601:5601\n\n  minio:\n    profiles:  # must pass one of these with --profile to docker-compose\n      - s3\n      - spark\n      - test\n    image: minio/minio:RELEASE.2022-04-12T06-55-35Z\n    container_name: minio\n    volumes:\n      - .:/dockermount\n      - type: bind\n        source: ${MINIO_DATA_DIR:-../data/s3}\n        target: /data\n    ports:\n      - ${MINIO_PORT:-10001}:10001\n      - ${MINIO_CONSOLE_PORT:-10002}:10002\n    environment:\n      MINIO_ACCESS_KEY: ${MINIO_ACCESS_KEY:-usaspending}\n      MINIO_SECRET_KEY: ${MINIO_SECRET_KEY:-usaspender}\n    entrypoint: >\n      /bin/sh -c \"\n        # Create the bucket within MinIO and file path for the data dict\n        mkdir -p data/dti-da-public-files-nonprod/user_reference_docs\n        # Create the bucket within MinIO used for endpoints that list generated downloads\n        mkdir -p data/bulk_download\n        cp dockermount/usaspending_api/data/Data_Dictionary_Crosswalk.xlsx data/dti-da-public-files-nonprod/user_reference_docs/Data_Dictionary_Crosswalk.xlsx\n        minio server --address \":10001\" --console-address \":10002\" /data\n      \"\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://${MINIO_HOST:-localhost}:${MINIO_PORT:-10001}/minio/health/live\"]\n      interval: 30s\n      timeout: 20s\n      retries: 3\n\n  spark-master:\n    profiles:\n      - spark  # must pass --profile spark to docker-compose for this to come up\n      - test\n    image: spark-base  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    # build context path needs to be relative to project root, from where docker-compose will be run\n    build:\n      context: .\n      dockerfile: Dockerfile.spark\n      args:\n        PROJECT_LOG_DIR: ${PROJECT_LOG_DIR}\n    container_name: spark-master\n    environment:\n      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST:-spark-master}\n      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT:-7077}\n      SPARK_MASTER_WEBUI_PORT: ${SPARK_MASTER_WEBUI_PORT:-4040}\n    command: >\n      /bin/sh -c \"\n        $${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.master.Master \\\n        --port $${SPARK_MASTER_PORT} \\\n        --webui-port $${SPARK_MASTER_WEBUI_PORT}\"\n    ports:\n      - ${SPARK_MASTER_PORT:-7077}:7077\n      - ${SPARK_MASTER_WEBUI_PORT:-4040}:4040\n    volumes:\n      - type: bind\n        source: .\n        target: /project\n        read_only: false\n\n  spark-worker:\n    profiles:\n      - spark  # must pass --profile spark to docker-compose for this to come up\n      - test\n    image: spark-base  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    # build context path needs to be relative to project root, from where docker-compose will be run\n    build:\n      context: .\n      dockerfile: Dockerfile.spark\n      args:\n        PROJECT_LOG_DIR: ${PROJECT_LOG_DIR}\n    container_name: spark-worker\n    depends_on:\n      - spark-master\n    environment:\n      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST:-spark-master}\n      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT:-7077}\n      SPARK_WORKER_WEBUI_PORT: ${SPARK_WORKER_WEBUI_PORT:-4041}\n    command: /bin/sh -c \"$${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port $${SPARK_WORKER_WEBUI_PORT} spark://$${SPARK_MASTER_HOST}:$${SPARK_MASTER_PORT}\"\n    ports:\n      - ${SPARK_WORKER_WEBUI_PORT:-4041}:4041\n    volumes:\n      - type: bind\n        source: .\n        target: /project\n        read_only: false\n\n  spark-history-server:\n    profiles:\n      - spark  # must pass --profile spark to docker-compose for this to come up\n      - test\n    image: spark-base  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    # build context path needs to be relative to project root, from where docker-compose will be run\n    build:\n      context: .\n      dockerfile: Dockerfile.spark\n      args:\n        PROJECT_LOG_DIR: ${PROJECT_LOG_DIR}\n    container_name: spark-history-server\n    environment:\n      SPARK_HISTORY_SERVER_PORT: ${SPARK_HISTORY_SERVER_PORT:-18080}\n    command: /bin/sh -c \"$${SPARK_HOME}/bin/spark-class org.apache.spark.deploy.history.HistoryServer\"\n    ports:\n      - ${SPARK_HISTORY_SERVER_PORT:-18080}:18080\n    volumes:\n      - type: bind\n        source: .\n        target: /project\n        read_only: false\n\n  # Example of running spark-submit container:\n  #  NOTE: double check package dependency versions here with those used in unit tests (conftest_spark.py), as these docs could have gotten stale\n  #  (1) Review config values in usaspending_api/config/envs/local.py and override any as needed in a .env file or -e environment variable\n  #  (2) Deploy minio in docker container (see README.md)\n  #  (3) Deploy the postgres DB docker container, if your script connects to a DB.\n  #      - If so, also export a JDBC_URL environment variable to pass in to the spark-submit container so it can find its connection\n  #  (4) If reading or writing to S3, make sure the bucket given by the value of config setting CONFIG.AWS_S3_BUCKET exists\n  #      - e.g. create via UI at http://localhost:10001\n  #      - or use MinIO client CLI: mc mb local/data\n  #  (4) Run the spark-submit container, citing the dependent packages:\n  #    (NOTEs:\n  #      - postgresql is needed as a JDBC driver, if connecting to a Postgres DB\n  #      - delta-core is needed to read/write in Delta Lake format\n  #      - hadoop-aws is needed for the S3AFileSystem.java, used to write data to S3,\n  #        - and should use the same hadoop version in your local setup\n  #        - NOTE that specifying hadoop-awas should pull in on its own the required version of the aws-java-sdk\n  #      - spark-hive is needed to use a hive metastore_db of schemas and tables\n  #        - the Docker image at this time only installs spark and hadoop standalone, which does not seem to include all the needed Hive jars\n  #\n  #    make docker-compose-run profiles=\"--profile spark\" args=\"--rm -e MINIO_HOST=minio -e JDBC_URL -e COMPONENT_NAME='My Spark Prototype Script' spark-submit \\\n  #      --packages org.postgresql:postgresql:42.2.23,io.delta:delta-core_2.12:1.2.1,org.apache.hadoop:hadoop-aws:3.3.1,org.apache.spark:spark-hive_2.12:3.2.1 \\\n  #      /project/usaspending_api/etl/tests/path_to_your_spark_prototype_script.py\"\n  spark-submit:\n    profiles:\n      - spark  # must pass --profile spark to docker-compose for this to come up\n      - test\n    image: spark-base  # when an image by this name is not found in the local repo, and it is forced to build, it will use this as the tag\n    # build context path needs to be relative to project root, from where docker-compose will be run\n    build:\n      context: .\n      dockerfile: Dockerfile.spark\n      args:\n        PROJECT_LOG_DIR: ${PROJECT_LOG_DIR}\n    container_name: spark-submit\n    depends_on:\n      - spark-master\n      - spark-worker\n      - spark-history-server\n      - minio\n    environment:\n      SPARK_MASTER_HOST: ${SPARK_MASTER_HOST:-spark-master}\n      SPARK_MASTER_PORT: ${SPARK_MASTER_PORT:-7077}\n      # i.e. target where host warehouse dir is bound in below volume config.\n      #   This env var needs to be picked up as the config for the spark.sql.warehouse.dir spark conf setting when SparkSessions are created inside of a spark-submitted job\n      SPARK_SQL_WAREHOUSE_DIR: /spark-warehouse\n      # i.e. a metastore_db sub dir of the target where host warehouse dir is bound in below volume config.\n      #   This env var needs to be picked up as the path part of the config for the spark.hadoop.javax.jdo.option.ConnectionURL spark conf setting when SparkSessions are created inside of a spark-submitted job\n      HIVE_METASTORE_DERBY_DB_DIR: /spark-warehouse/metastore_db\n      PYTHONPATH: \"/project\"\n    # NOTE: entrypoint CANNOT interpolate env vars when processed. They are passed through literally.\n    # So in using 1 $ rather than 2 $$, the var is evaluated based on the current SHELL ENV when docker-compose is run,\n    # and interpolated before accessed as the entrypoint.\n    # While this service has values for these interpolated vars in the environment: element, those are not used here,\n    # but merely passed into the container. KEEP the two references to these vars and their defaults consistent!\n    # To see what it will be, you can run docker-compose config (i.e. make docker-compose-config in this project's Makefile)\n    entrypoint: ./bin/spark-submit --master spark://${SPARK_MASTER_HOST:-spark-master}:${SPARK_MASTER_PORT:-7077}\n    command: --help\n    volumes:\n      - type: bind\n        source: .\n        target: /project\n        read_only: false\n      # NOTE: The hive metastore_db Derby database folder is expected to be configured to show up as a subfolder of the spark-warehouse dir\n      - type: bind\n        source: ${SPARK_SQL_WAREHOUSE_DIR:-./spark-warehouse}\n        target: /spark-warehouse\n      # Mount the JAR dependencies local repo on host into container to take advantage of caching/reuse\n      # i.e., to download the dependencies only once and reuse on subsequent docker-compose run calls\n      - type: bind\n        source: ${HOME}/.ivy2\n        target: /root/.ivy2\n        read_only: false\n",
            "Dockerfile": "# Dockerfile for the USAspending Backend API\n# When built with docker-compose --profile usaspending build,\n# it will be built and tagged with the name in the image: key of the docker-compose services that use this default Dockerfile\n\n# Since requirements are copied into the image at build-time, this MUST be rebuilt if Python requirements change\n\n# See docker-compose.yml file and README.md for docker-compose information\n\nFROM python:3.10.12-slim-bullseye\n\nWORKDIR /dockermount\n\nRUN apt update && \\\n    apt install -y gcc postgresql-13 libpq-dev\n\n##### Copy python packaged\nWORKDIR /dockermount\n# COPY requirements/ /dockermount/requirements/\n\nCOPY . /dockermount\nRUN python3 -m pip install -r requirements/requirements.txt && \\\n    python3 -m pip install -r requirements/requirements-server.txt && \\\n    python3 -m pip install ansible==2.9.15 awscli==1.34.19\n\n##### Ensure Python STDOUT gets sent to container logs\nENV PYTHONUNBUFFERED=1\n"
        },
        "static_files": {}
    },
    "project_structure": {
        "files": [
            ".codeclimate.yml",
            ".env.template",
            ".gitignore",
            ".pre-commit-config.yaml",
            ".pyup.yml",
            ".travis.yml",
            "CONTRIBUTING.md",
            "Dockerfile",
            "Dockerfile.spark",
            "Dockerfile.testing",
            "LICENSE",
            "MANIFEST.in",
            "Makefile",
            "README.md",
            "__init__.py",
            "data_reformatting.md",
            "docker-compose.yml",
            "dredd.yml",
            "loading_data.md",
            "manage.py",
            "pyproject.toml",
            "readme.jpg",
            "setup.cfg",
            "setup.py"
        ],
        "folders": [
            ".github",
            "bulk_downloads",
            "config",
            "csv_downloads",
            "requirements",
            "usaspending_api"
        ]
    }
}