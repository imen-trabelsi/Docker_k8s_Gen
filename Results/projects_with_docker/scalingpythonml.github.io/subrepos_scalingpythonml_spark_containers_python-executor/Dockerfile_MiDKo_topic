# Get Base Image
FROM $base_img

WORKDIR /

# System dependencies
RUN apt-get update --fix-missing && \

# Install bash tini & other dependencies
RUN apt-get install -y bash tini libc6 libpam-modules krb5-user libnss3 procps ca-certificates p11-kit wget bzip2 git mercurial subversion && \
    rm /bin/sh && \

# Copy neccessary files
COPY bin/pysetup.sh /pysetup.sh
ADD https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar ${SPARK_HOME}/jars/
ADD https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.563/aws-java-sdk-bundle-1.11.563.jar ${SPARK_HOME}/jars/
ADD https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark3-runtime/0.11.0/iceberg-spark3-runtime-0.11.0.jar ${SPARK_HOME}/jars/
ADD https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark3-extensions/0.11.0/iceberg-spark3-extensions-0.11.0.jar ${SPARK_HOME}/jars/
ADD https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark3/0.11.0/iceberg-spark3-0.11.0.jar ${SPARK_HOME}/jars/
COPY python/pyspark ${SPARK_HOME}/python/pyspark
COPY python/lib ${SPARK_HOME}/python/lib

# Create PySpark directory
RUN mkdir ${SPARK_HOME}/python

# Install python and python packages
RUN chmod a+x /pysetup.sh && ./pysetup.sh
RUN conda install -c conda-forge --yes mamba &&\
    mamba install --yes python==3.8.6 &&\
    pip install --upgrade pip setuptools &&\
    mamba install --yes numpy==1.19.2 pandas cytoolz numba lz4 scikit-build python-blosc=1.9.2 &&\

# Set correct permissions
RUN chmod a+rx ${SPARK_HOME}/jars/*.jar

# Changing Working directory
WORKDIR /opt/spark/work-dir

# Environment Variables
ENV PATH /opt/conda/bin:$PATH

# Creating users
USER ${spark_uid}

# Execution Command
CMD ["/opt/entrypoint.sh"]
