{
    "source_code_info": {
        "language": "Ruby",
        "framework": "Ruby",
        "dependencies": {
            "Gemfile": "source \"https://rubygems.org\"\n# Hello! This is where you manage which Jekyll version is used to run.\n# When you want to use a different version, change it below, save the\n# file and run `bundle install`. Run Jekyll with `bundle exec`, like so:\n# Note: you probably want to set a local bundle path with : /usr/local/bin/bundle config set path 'vendor/bundle'\n#\n#     bundle exec jekyll serve\n#\n# This will help ensure the proper Jekyll version is running.\n# Happy Jekylling!\n# gem \"jekyll\", \"~> 4.0.0\"\n# This is the default theme for new Jekyll sites. You may change this to anything you like.\ngem \"minima\", \"~> 2.5\"\n# To upgrade, run `bundle update github-pages`.\ngem \"github-pages\", group: :jekyll_plugins\n# If you have any plugins, put them here!\ngroup :jekyll_plugins do\n  gem \"jekyll-feed\", \"~> 0.12\"\n  gem 'jekyll-octicons'\nend\n\ngem \"jekyll-github-metadata\"\n\n# Windows and JRuby does not include zoneinfo files, so bundle the tzinfo-data gem\n# and associated library.\ninstall_if -> { RUBY_PLATFORM =~ %r!mingw|mswin|java! } do\n  gem \"tzinfo\", \"~> 1.2\"\n  gem \"tzinfo-data\"\nend\n\n# Performance-booster for watching directories on Windows\ngem \"wdm\", \"~> 0.1.1\", :install_if => Gem.win_platform?\n\ngem \"faraday\", \"< 1.0\"\n\n# I like asciidoc & sitemaps\ngroup :jekyll_plugins do\n  gem 'jekyll-asciidoc'\n  gem 'jekyll-sitemap'\nend\n"
        },
        "database": "PostgreSQL",
        "build_system": "Bundler",
        "config_files": {
            "subrepos/docker-stacks/pyspark-notebook/Dockerfile": "# Copyright (c) Jupyter Development Team.\n# Distributed under the terms of the Modified BSD License.\nARG BASE_CONTAINER=jupyter/scipy-notebook\nFROM $BASE_CONTAINER\n\nLABEL maintainer=\"Jupyter Project <jupyter@googlegroups.com>\"\n\n# Fix DL4006\nSHELL [\"/bin/bash\", \"-o\", \"pipefail\", \"-c\"]\n\nUSER root\n\n# Spark dependencies\n# Default values can be overridden at build time\n# (ARGS are in lower case to distinguish them from ENV)\nARG spark_version=\"3.0.1\"\nARG hadoop_version=\"3.2\"\nARG spark_checksum=\"E8B47C5B658E0FBC1E57EEA06262649D8418AE2B2765E44DA53AAF50094877D17297CC5F0B9B35DF2CEEF830F19AA31D7E56EAD950BBE7F8830D6874F88CFC3C\"\nARG openjdk_version=\"11\"\n\nENV APACHE_SPARK_VERSION=\"${spark_version}\" \\\n    HADOOP_VERSION=\"${hadoop_version}\"\n\nRUN apt-get -y update && \\\n    apt-get install --no-install-recommends -y \\\n    \"openjdk-${openjdk_version}-jre-headless\" \\\n    ca-certificates-java && \\\n    apt-get clean && rm -rf /var/lib/apt/lists/*\n\n# Spark installation\nWORKDIR /tmp\n# Using the preferred mirror to download Spark\n# hadolint ignore=SC2046\nRUN wget -q $(wget -qO- https://www.apache.org/dyn/closer.lua/spark/spark-${APACHE_SPARK_VERSION}/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\\?as_json | \\\n    python -c \"import sys, json; content=json.load(sys.stdin); print(content['preferred']+content['path_info'])\") && \\\n    echo \"${spark_checksum} *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha512sum -c - && \\\n    tar xzf \"spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" -C /usr/local --owner root --group root --no-same-owner && \\\n    rm \"spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\"\n\nWORKDIR /usr/local\n\n# Configure Spark\nENV SPARK_HOME=/usr/local/spark\nENV SPARK_OPTS=\"--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info\" \\\n    PATH=$PATH:$SPARK_HOME/bin\n\nRUN ln -s \"spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}\" spark && \\\n    # Add a link in the before_notebook hook in order to source automatically PYTHONPATH\n    mkdir -p /usr/local/bin/before-notebook.d && \\\n    ln -s \"${SPARK_HOME}/sbin/spark-config.sh\" /usr/local/bin/before-notebook.d/spark-config.sh\n\n# Fix Spark installation for Java 11 and Apache Arrow library\n# see: https://github.com/apache/spark/pull/27356, https://spark.apache.org/docs/latest/#downloading\nRUN cp -p \"$SPARK_HOME/conf/spark-defaults.conf.template\" \"$SPARK_HOME/conf/spark-defaults.conf\" && \\\n    echo 'spark.driver.extraJavaOptions=\"-Dio.netty.tryReflectionSetAccessible=true\"' >> $SPARK_HOME/conf/spark-defaults.conf && \\\n    echo 'spark.executor.extraJavaOptions=\"-Dio.netty.tryReflectionSetAccessible=true\"' >> $SPARK_HOME/conf/spark-defaults.conf\n\nUSER $NB_UID\n\n# Install pyarrow\nRUN conda install --quiet --yes --satisfied-skip-solve \\\n    'pyarrow=2.0.*' && \\\n    conda clean --all -f -y && \\\n    fix-permissions \"${CONDA_DIR}\" && \\\n    fix-permissions \"/home/${NB_USER}\"\n\nWORKDIR $HOME\n"
        },
        "static_files": {}
    },
    "project_structure": {
        "files": [
            "Dockerfile",
            "prepare.sh"
        ],
        "folders": [
            "examples"
        ]
    }
}